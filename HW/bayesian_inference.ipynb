{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "In this exercise you will implement a method for inferring the posterior probability density of the variance of thedistribution from which a set of samples were drawn. You will use Bayes' theorem to derive a posterior probabilitydistribution for this variance. You will then proceed, in a rather gratuitous application of the Metropolis-Hastingalgorithm, to sample from this posterior density to form a Markov chain. We then use the Markov chain to estimatethe posterior density. I call this application 'rather gratuitous' because we already know the posterior densityand can plot it and do anything we want with it. The utility of generating a Markov chain will become more evidentin the group project.\n",
    "\n",
    "1) Assume a Gaussian random number generator with zero mean and variance a is producing samples, x,  with posterior density P(x|a) = N(a) e^{-x^2/(2a)}. Analytically find N(a) so that P(x|a) is appropriately normalized.\n",
    "\n",
    "3) Use Bayesâ€™s theorem to calculate P(a|{x}) when one has multiple samples drawn;  {x} =  (x_1, x_2, x_3, ... x_n).\n",
    "\n",
    "4) Draw 100 samples and plot the resulting P(a|{x}).\n",
    "\n",
    "5) Use the Metropolis Hastings algorithm to sample from this posterior and create a Markov chain.\n",
    "\n",
    "6) Plot a \"trace plot\" which is sample number vs. parameter value.\n",
    "\n",
    "7) Plot a histogram of the chain with variance a as the x axis and compare with P(a|x). Indicate in your graph the true value of the variance.\n",
    "\n",
    "Develop in the VS Code IDE under version control on your own GitHub repo. Submit a link to the GitHub repo.\n",
    "\n",
    "\n",
    "\n",
    "NOTE:\n",
    "\n",
    "1) In (3), simply adopt a uniform prior, which means P(a) is independent of a.\n",
    "\n",
    "2) In (3), you only need to calculate an un-normalized P(a|x); i.e., don't concern yourself with factors that have\n",
    "no dependence on a. If you do want to normalize P(a|x) you can do so by making sure that \\int da P(a|x)=1 -- but\n",
    "you don't have to."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.integrate as integrate"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define width and mean of Gaussian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "width = 1\n",
    "mean = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Define the gaussian with normalization, it should equal 1 when integrated over all x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def gaussian(x):\n",
    "    return 1 / (np.sqrt(2 * np.pi) * width) * np.exp(-(x - mean) ** 2 / (2 * width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Integrate using scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9999999999999997, 1.017819145094224e-08)\n"
     ]
    }
   ],
   "source": [
    "print(integrate.quad(gaussian, -np.inf, np.inf))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "When you have multiple samples, the probability function becomes\n",
    "\n",
    "$ P(x_1, ..., x_n|\\sigma) = \\prod^{\\inf}_{i=1} P(x_i|\\sigma)=N^n(\\sigma) e^{-\\sum_i (x_i-\\mu)^2/(2 \\sigma^2)}$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of data: [-0.50793531  0.1014849  -0.82847935 -0.04663276 -1.15313846 -0.36921136\n",
      "  0.45134089 -0.79851717 -0.41550234  0.39220178]\n",
      "normalization=0.3989422804014327\n",
      "n_normalization=1.233123522100347e-40\n",
      "sum_of_squared_data=112.69050717757152\n",
      "exponential=3.3850671242038982e-25\n",
      "probability=4.174205894744404e-65\n"
     ]
    }
   ],
   "source": [
    "# Make one hundred samples\n",
    "data = np.random.normal(0, 1, 100)\n",
    "print(f\"Sample of data: {data[:10]}\")\n",
    "\n",
    "# For one Gaussian, the normalization is...\n",
    "normalization = 1/(np.sqrt(2*np.pi)*width)\n",
    "print(f\"{normalization=}\")\n",
    "\n",
    "# For one hundred points, do N(sigma)^n\n",
    "n_normalization = normalization ** 100\n",
    "print(f\"{n_normalization=}\")\n",
    "\n",
    "# The exponent has a sum over the squares of all the data\n",
    "squared_data = [(i - mean)**2 for i in data]\n",
    "sum_of_squared_data = sum(squared_data)\n",
    "print(f\"{sum_of_squared_data=}\")\n",
    "\n",
    "# Final value of exponential\n",
    "exponential = np.exp(-sum_of_squared_data / (2*width**2))\n",
    "print(f\"{exponential=}\")\n",
    "\n",
    "# Multiply times normalization\n",
    "probability = n_normalization * exponential\n",
    "print(f\"{probability=}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "rai",
   "language": "python",
   "display_name": "Rai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}